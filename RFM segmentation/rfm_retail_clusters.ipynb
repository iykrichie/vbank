{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b91c7a9-befe-4693-9e60-2b337c7602b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# RFM Classifications for Retail Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200d62c7-bc16-4a25-a680-21f7cb3bf7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Introduction\n",
    "RFM segmentation is a powerful marketing technique used to analyze and segment customers based on their past behavior. RFM stands for Recency, Frequency, and Monetary Value, which are three key metrics used to understand customer engagement and value.\n",
    "\n",
    "In this notebook, we will explore how to perform RFM segmentation using Python and popular data analysis libraries such as pandas and scikit-learn. We will analyze transactional data for retail clients within a 12-month period to demonstrate the process of RFM segmentation and derive actionable insights for targeted marketing strategies.\n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8b7adc-dfb6-4d6f-a468-b180590f0826",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import json\n",
    "with open('/Workspace/Credentials/db_data.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "\n",
    "host = data['redshift']['host']\n",
    "user = data['redshift']['user']\n",
    "passwd = data['redshift']['passwd']\n",
    "database = data['redshift']['database']\n",
    "\n",
    "conn = create_engine(f\"postgresql+psycopg2://{user}:{passwd}@{host}:5439/{database}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d2e7ca-512a-44a6-b36f-2ec52c6bd4c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Code Documentation: Connecting to a Redshift Database using SQLAlchemy\n",
    "\n",
    "## Libraries Used\n",
    "- `sqlalchemy`: Used to create an engine for database connections.\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "- `numpy`: Used for numerical operations.\n",
    "- `matplotlib.pyplot` and `seaborn`: Used for data visualization.\n",
    "- `json`: Used to read JSON files for database credentials.\n",
    "\n",
    "## Code Explanation\n",
    "1. **Import Libraries:** Import necessary libraries including SQLAlchemy, pandas, numpy, matplotlib.pyplot, seaborn, and json.\n",
    "\n",
    "2. **Read Database Credentials:** Read database connection credentials (host, user, password, database name) from a JSON file (`db_data.json` in this case).\n",
    "\n",
    "3. **Create Database Connection String:** Using the credentials read from the JSON file, create a connection string for the Redshift database using SQLAlchemy's `create_engine` function. The connection string follows the format `postgresql+psycopg2://user:password@host:port/database_name`.\n",
    "\n",
    "4. **Establish Database Connection:** Use the created connection string to establish a connection to the Redshift database using the `create_engine` function.\n",
    "\n",
    "## Notes\n",
    "- Ensure that the `json` file containing database credentials (`db_data.json` in this case) is present in the specified path (`/Workspace/Credentials/`) and follows the correct JSON format with keys for `host`, `user`, `passwd`, and `database`.\n",
    "\n",
    "- Modify the connection string (`f\"postgresql+psycopg2://{user}:{passwd}@{host}:5439/{database}\"`) as per your database configuration, such as port number or additional parameters.\n",
    "\n",
    "- Once the connection is established (`conn` object), you can use it to execute SQL queries, fetch data into pandas DataFrames, and perform data analysis or visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec32587e-0446-4656-b001-60181e32321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "yesterday =  (datetime.today() - timedelta(days = 1)).strftime('%Y-%m-%d')\n",
    "print(today)\n",
    "print(yesterday)\n",
    "\n",
    "\n",
    "last_2_wks = datetime.today() - timedelta(days = 14)\n",
    "last_2_wks = last_2_wks.strftime('%Y-%m-%d')\n",
    "print('------------------------------------')\n",
    "print(last_2_wks)\n",
    "\n",
    "print('\\n')\n",
    "now = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "last_30_mins = (datetime.today() - timedelta(days = 1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "trunc_last_30_mins = (datetime.today() - timedelta(days = 1)).strftime('%Y-%m-%d %H:%M')\n",
    "print(last_30_mins, 'to', now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53f2921-7c8c-4414-b0ac-7a4a8e6ba75d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Code Documentation: Setting Pandas Options and Date Manipulation\n",
    "\n",
    "## Libraries Used\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "- `datetime`: Used for date and time manipulation.\n",
    "\n",
    "## Code Explanation\n",
    "1. **Setting Pandas Option:** Using `pd.set_option('display.float_format', lambda x: '%.2f' % x)` to set the display format for floating-point numbers in pandas DataFrames. This option ensures that floating-point numbers are displayed with two decimal places.\n",
    "\n",
    "2. **Date Manipulation:** \n",
    "    - `datetime.today()`: Get the current date and time.\n",
    "    - `datetime.today().strftime('%Y-%m-%d')`: Format the current date as 'YYYY-MM-DD'.\n",
    "    - `timedelta(days=1)`: Create a timedelta object representing one day.\n",
    "    - `datetime.today() - timedelta(days=1)`: Get yesterday's date.\n",
    "    - `datetime.today() - timedelta(days=14)`: Get the date 14 days ago.\n",
    "    - `strftime('%Y-%m-%d')`: Format dates as 'YYYY-MM-DD'.\n",
    "    - `strftime('%Y-%m-%d %H:%M:%S')`: Format dates with date and time as 'YYYY-MM-DD HH:MM:SS'.\n",
    "    - Example outputs are printed to demonstrate these operations.\n",
    "\n",
    "## Notes\n",
    "- The `pd.set_option('display.float_format', lambda x: '%.2f' % x)` command ensures that floating-point numbers in pandas DataFrames are displayed with two decimal places. Modify the format string (`'%.2f'`) as needed for different display requirements.\n",
    "\n",
    "- Adjust the date manipulation operations (`timedelta`, `strftime`, etc.) based on your specific date and time requirements. These examples demonstrate common date manipulations like getting today's date, yesterday's date, and date ranges.\n",
    "\n",
    "- Ensure the datetime format strings (`'%Y-%m-%d'`, `'%Y-%m-%d %H:%M:%S'`, etc.) match the desired output format for your application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a153c2ca-b192-4fc2-b566-23e79351f5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = '''\n",
    "SELECT\n",
    "    dac.client_id,\n",
    "    dac.client_category,\n",
    "    COUNT(dat.transaction_id) AS Frequency,\n",
    "    SUM(dat.amount) AS Monetary,\n",
    "    DATEDIFF(DAYS, MAX(dat.transaction_date), CURRENT_DATE) AS Recency,\n",
    "    DATEDIFF(DAYS, MIN(dac.activation_date), CURRENT_DATE) AS T,\n",
    "    CURRENT_DATE AS rundate\n",
    "FROM\n",
    "    dwh_all_transactions dat\n",
    "LEFT JOIN\n",
    "    dwh_all_clients dac ON dat.client_id = dac.client_id\n",
    "LEFT JOIN\n",
    "    dwh_all_accounts daa ON dac.client_id = daa.client_id\n",
    "WHERE\n",
    "    dat.transaction_type_enum IN (1, 2)\n",
    "    AND UPPER(dac.client_category) IN ('RETAIL CLIENT')\n",
    "    AND dac.client_status = 'Active'\n",
    "    AND dat.transaction_date >= DATEADD(MONTH, -12, CURRENT_DATE)  -- Assuming a SQL-compatible syntax for date manipulation\n",
    "GROUP BY\n",
    "    dac.client_id,\n",
    "    dac.client_category\n",
    "HAVING COUNT(dat.transaction_id) > 0;\n",
    "'''\n",
    "\n",
    "# Execute the query using the engine and read the result into a DataFrame\n",
    "query_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(query_data.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce322d8e-6ffe-45b9-b33c-c3618fa0be3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Code Documentation: Executing SQL Query and Reading Result into Pandas DataFrame\n",
    "\n",
    "## Libraries Used\n",
    "- `sqlalchemy`: Used to create an engine for database connections.\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "\n",
    "## Code Explanation\n",
    "1. **SQL Query Definition:** Define an SQL query to retrieve data from a database. The query calculates RFM (Recency, Frequency, Monetary) metrics for retail clients within the last 12 months, based on transaction data and client information.\n",
    "\n",
    "2. **Execution and Data Retrieval:**\n",
    "    - `pd.read_sql_query(query, conn)`: Execute the SQL query using the established database connection (`conn`) and read the result into a pandas DataFrame (`query_data`).\n",
    "    - The SQL query calculates Frequency (transaction count), Monetary (sum of transaction amounts), Recency (days since last transaction), and T (days since client activation) for active retail clients with specific transaction types and statuses.\n",
    "\n",
    "3. **Display Data:** Print the first few rows (`head(3)`) of the resulting DataFrame (`query_data`) to inspect the data retrieved from the database.\n",
    "\n",
    "## Notes\n",
    "- Ensure that the SQL query (`query`) is correctly formatted and compatible with the database engine being used (PostgreSQL in this case).\n",
    "\n",
    "- Modify the SQL query as per your specific data requirements, such as changing date conditions, filtering criteria, or aggregation logic.\n",
    "\n",
    "- Adjust the number of rows displayed (`head(n)`) based on the size of the dataset and your inspection needs.\n",
    "\n",
    "- The result stored in `query_data` can be further analyzed, visualized, or used for machine learning models within the Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f75c618-3860-4a82-90d2-579a87c60d67",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dropping duplicate rows\n",
    "\n",
    "query_data.drop_duplicates()\n",
    "\n",
    "df = query_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61752c5-cae2-40da-ac1c-f77a20311190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Documentation: Dropping Duplicate Rows from a DataFrame\n",
    "\n",
    "## Libraries Used\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "\n",
    "## Code Explanation\n",
    "1. **Drop Duplicate Rows:**\n",
    "    - `df.drop_duplicates()`: Removes duplicate rows from the DataFrame `df`. Duplicate rows are identified based on all columns having identical values across rows. Only the first occurrence of a duplicate row is kept, and subsequent duplicates are removed.\n",
    "\n",
    "2. **Assignment:**\n",
    "    - `df = ...`: Assign the result of dropping duplicate rows back to the DataFrame `df` or to a new DataFrame if needed.\n",
    "\n",
    "## Notes\n",
    "- Dropping duplicate rows helps in cleaning and preparing data for analysis, especially when working with large datasets or merging multiple datasets.\n",
    "\n",
    "- Ensure that the DataFrame (`df`) contains data and is appropriately loaded or created before dropping duplicates.\n",
    "\n",
    "- Modify the `subset` parameter within `drop_duplicates()` to specify columns for identifying duplicates if needed. By default, it considers all columns for duplicate detection.\n",
    "\n",
    "- Use caution when dropping duplicates, as it permanently modifies the DataFrame unless assigned to a new variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3763c108-5900-4c18-805a-a5b6cfbe8d73",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['recency'].plot.box() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890340b6-e941-42d1-8a83-4ebb70b58071",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['recency'])\n",
    "#plt.savefig('plt/DaysSinceLastTx.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febed450-7c73-4e96-940c-eb13618ed95f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['frequency'].plot.box() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef861ad4-009d-4b2c-b920-cfeaf34e9d77",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['frequency'])\n",
    "#plt.savefig('plt/txcount.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a4ea9b-0097-4319-a727-66e09115b6bb",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['monetary'].plot.box() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a200ac4f-c4b6-4c4d-8e08-3d93d6b9fa14",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['monetary'])\n",
    "#plt.savefig('plt/revenue.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e34609-ecb0-4004-bd04-ff90d48b849f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e060d515-5d4e-4a39-9f0a-14d4bd3b1e11",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extracting columns client_id, recency, frequency, monetary\n",
    "rfm = df[['client_id', 'recency', 'frequency', 'monetary']].copy()\n",
    "\n",
    "# Displaying the first few rows of sdf\n",
    "print(rfm.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c255eca-72c7-4310-9339-573c4cdd4ada",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae46f19-4acf-49ba-9b29-808b30caa6db",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quintiles = rfm[['recency', 'frequency', 'monetary']].quantile([.2, .4, .6, .8]).to_dict()\n",
    "quintiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f30b4bf-d1bd-49a7-ae23-996e5110ef75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Documentation: Calculating RFM Quintiles\n",
    "\n",
    "## Libraries Used\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "\n",
    "## Code Explanation\n",
    "1. **Calculate RFM Quintiles:**\n",
    "    - `rfm[['recency', 'frequency', 'monetary']].quantile([.2, .4, .6, .8])`: Calculate quintiles (20th, 40th, 60th, and 80th percentiles) for the RFM metrics (Recency, Frequency, Monetary) using pandas `quantile` function. This divides the data into five equal parts, forming quintiles.\n",
    "    - `.to_dict()`: Convert the calculated quintiles into a dictionary format for easy access and usage.\n",
    "\n",
    "2. **Assign Quintiles:**\n",
    "    - `vquintiles = ...`: Assign the calculated quintiles dictionary to the variable `vquintiles` for later use in RFM analysis or segmentation.\n",
    "\n",
    "## Notes\n",
    "- Quintiles are used to divide a dataset into five equal parts based on percentile ranks. The specified percentiles (.2, .4, .6, .8) divide the data into quintiles with 20%, 40%, 60%, and 80% of the data falling below each respective threshold.\n",
    "\n",
    "- Adjust the percentiles or metrics (`['recency', 'frequency', 'monetary']`) as needed for your specific RFM analysis requirements.\n",
    "\n",
    "- Ensure that the `rfm` DataFrame contains the necessary RFM metrics ('recency', 'frequency', 'monetary') for calculating quintiles.\n",
    "\n",
    "- Use the `vquintiles` variable to access the calculated quintiles in subsequent RFM segmentation or analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd56533-a5d0-4e82-b7c3-3abf57173a79",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def r_score(x):\n",
    "    if x <= quintiles['recency'][.2]:\n",
    "        return 5\n",
    "    elif x <= quintiles['recency'][.4]:\n",
    "        return 4\n",
    "    elif x <= quintiles['recency'][.6]:\n",
    "        return 3\n",
    "    elif x <= quintiles['recency'][.8]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def fm_score(x, c):\n",
    "    if x <= quintiles[c][.2]:\n",
    "        return 1\n",
    "    elif x <= quintiles[c][.4]:\n",
    "        return 2\n",
    "    elif x <= quintiles[c][.6]:\n",
    "        return 3\n",
    "    elif x <= quintiles[c][.8]:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab94092-dd6b-4440-8709-118e85f301bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Documentation: RFM Scoring Functions\n",
    "\n",
    "## Functions Overview\n",
    "1. **`r_score(x)` Function:**\n",
    "    - Assigns an RFM (Recency) score based on the input `x` value (recency metric).\n",
    "    - Uses predefined quintiles for recency to determine the score:\n",
    "        - If `x` is in the top 20%, returns a score of 5.\n",
    "        - If `x` is in the 20-40% range, returns a score of 4.\n",
    "        - If `x` is in the 40-60% range, returns a score of 3.\n",
    "        - If `x` is in the 60-80% range, returns a score of 2.\n",
    "        - Otherwise, returns a score of 1.\n",
    "\n",
    "2. **`fm_score(x, c)` Function:**\n",
    "    - Assigns an RFM (Frequency/Monetary) score based on the input `x` value (frequency or monetary metric) and category `c` (frequency or monetary).\n",
    "    - Uses predefined quintiles for the specified category to determine the score:\n",
    "        - If `x` is in the top 20%, returns a score of 1.\n",
    "        - If `x` is in the 20-40% range, returns a score of 2.\n",
    "        - If `x` is in the 40-60% range, returns a score of 3.\n",
    "        - If `x` is in the 60-80% range, returns a score of 4.\n",
    "        - Otherwise, returns a score of 5.\n",
    "\n",
    "## Notes\n",
    "- These scoring functions are used in RFM segmentation to assign scores to customers based on their recency, frequency, and monetary values relative to predefined quintiles.\n",
    "- Modify the scoring logic or quintiles as needed to match specific business requirements or segmentation strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ea3aed-f613-49ab-8fd1-923506b7bec0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfm['R'] = rfm['recency'].apply(lambda x: r_score(x))\n",
    "rfm['F'] = rfm['frequency'].apply(lambda x: fm_score(x, 'frequency'))\n",
    "rfm['M'] = rfm['monetary'].apply(lambda x: fm_score(x, 'monetary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c696aa7-34bb-4d53-89e0-11b35475e0da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Documentation: Applying RFM Scoring Functions to RFM DataFrame\n",
    "\n",
    "## Code Explanation\n",
    "1. **Assigning RFM Scores:**\n",
    "    - `rfm['R'] = rfm['recency'].apply(lambda x: r_score(x))`: Applies the `r_score` function to the 'recency' column of the RFM DataFrame (`rfm`) and assigns the resulting scores to a new column 'R' representing recency scores.\n",
    "    \n",
    "    - `rfm['F'] = rfm['frequency'].apply(lambda x: fm_score(x, 'frequency'))`: Applies the `fm_score` function to the 'frequency' column of the RFM DataFrame (`rfm`) and assigns the resulting scores to a new column 'F' representing frequency scores.\n",
    "\n",
    "    - `rfm['M'] = rfm['monetary'].apply(lambda x: fm_score(x, 'monetary'))`: Applies the `fm_score` function to the 'monetary' column of the RFM DataFrame (`rfm`) and assigns the resulting scores to a new column 'M' representing monetary scores.\n",
    "\n",
    "## Notes\n",
    "- These lines of code apply the previously defined scoring functions (`r_score` and `fm_score`) to calculate and assign RFM scores for each customer in the RFM DataFrame (`rfm`).\n",
    "\n",
    "- The `apply` method is used to apply the scoring functions element-wise to the specified columns ('recency', 'frequency', 'monetary') of the DataFrame.\n",
    "\n",
    "- Ensure that the DataFrame (`rfm`) contains the necessary columns ('recency', 'frequency', 'monetary') before applying the scoring functions.\n",
    "\n",
    "- The resulting 'R', 'F', and 'M' columns in the DataFrame represent the recency, frequency, and monetary scores respectively, calculated based on predefined quintiles and scoring logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7566f55-605f-4061-ab8f-5bfa945c34d5",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfm['RFMScore'] = rfm['R'].map(str) + rfm['F'].map(str) + rfm['M'].map(str)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f01de9-4d70-4906-9138-49d581f9751c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Documentation: Creating RFM Score and Updating DataFrame\n",
    "\n",
    "## Code Explanation\n",
    "1. **Creating RFM Score:**\n",
    "    - `rfm['RFM Score'] = rfm['R'].map(str) + rfm['F'].map(str) + rfm['M'].map(str)`: Concatenates the 'R', 'F', and 'M' columns as strings to create an RFM score for each customer. This new column 'RFM Score' represents the combined RFM scores for segmentation purposes.\n",
    "\n",
    "2. **DataFrame Update:**\n",
    "    - `rfm.head()`: Displays the first few rows of the updated DataFrame (`rfm`) including the newly created 'RFM Score' column.\n",
    "\n",
    "## Notes\n",
    "- The code combines the individual R, F, and M scores as strings to create a composite RFM score for each customer, facilitating segmentation based on RFM scoring.\n",
    "\n",
    "- Ensure that the DataFrame (`rfm`) contains the 'R', 'F', and 'M' columns representing recency, frequency, and monetary scores before creating the RFM score.\n",
    "\n",
    "- The resulting 'RFM Score' column contains concatenated strings representing the RFM scores for each customer, combining their recency, frequency, and monetary scores into a single metric for segmentation analysis.\n",
    "\n",
    "- Adjust the display settings (`head()`) or perform further analysis as needed based on the updated DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68e211-fb12-4495-a9f2-b5fcdbc9a03a",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "segt_map = {\n",
    "    r'[1-2][1-2]': 'Hibernating',\n",
    "    r'[1-2][3-4]': 'At Risk',\n",
    "    r'[1-2]5': 'Cant Loose',\n",
    "    r'3[1-2]': 'About To Sleep',\n",
    "    r'33': 'Need Attention',\n",
    "    r'[3-4][4-5]': 'Loyal Customers',\n",
    "    r'41': 'Promising',\n",
    "    r'51': 'New Customers',\n",
    "    r'[4-5][2-3]': 'Potential Loyalists',\n",
    "    r'5[4-5]': 'Champions'\n",
    "}\n",
    "\n",
    "rfm['Segment'] = rfm['R'].map(str) + rfm['F'].map(str)\n",
    "rfm['Segment'] = rfm['Segment'].replace(segt_map, regex=True)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0fd1c8e-ec4e-4d5a-b779-d8fea273be3a",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count the number of customers in each segment\n",
    "segments_counts = rfm['Segment'].value_counts().sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bars = ax.barh(range(len(segments_counts)),\n",
    "              segments_counts,\n",
    "              color='silver')\n",
    "ax.set_frame_on(False)\n",
    "ax.tick_params(left=False,\n",
    "               bottom=False,\n",
    "               labelbottom=False)\n",
    "ax.set_yticks(range(len(segments_counts)))\n",
    "ax.set_yticklabels(segments_counts.index)\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "        value = bar.get_width()\n",
    "        if segments_counts.index[i] in ['Champions', 'Loyal Customers']:\n",
    "            bar.set_color('green')\n",
    "        ax.text(value,\n",
    "                bar.get_y() + bar.get_height()/2,\n",
    "                '{:,} ({:}%)'.format(int(value),\n",
    "                                   int(value*100/segments_counts.sum())),\n",
    "                va='center',\n",
    "                ha='left'\n",
    "               )\n",
    "#plt.savefig('plt/rfmsegments.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f48135-d6bf-43d9-ac2e-9edfefa1a3f6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of the RFM Segments\n",
    "\n",
    "sns.distplot(rfm['RFMScore'])\n",
    "#plt.savefig('plt/rfm_score.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db33cb33-26f0-4d61-9efb-c45edfba4ed0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# rfm.to_csv('data/rfm_clusters_2023h1.csv')\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dac4cb-94a2-44db-bbf5-848cebd5a0a9",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfm['RunDate'] = pd.to_datetime('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3298aaee-de29-4c08-8fc7-4de8ef1ac876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = rfm \n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de20ec7f-ab5e-439d-8433-99d6a42d4eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession if not already created\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Write Spark DataFrame to table in Databricks\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"vfd_databricks.default.retail_rfm_clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781c4e95-1352-4813-b454-767a83914bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "DROP TABLE vfd_databricks.default.retail_rfm_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401606f9-1378-473a-8c8f-a29e5efdba52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Write DataFrame to Redshift\n",
    "# Assuming the table name should be 'dwh_rfm_clusters'\n",
    "table_name = 'dwh_retail_rfm_clusters'\n",
    "\n",
    "# Write the DataFrame to the Redshift table\n",
    "df.to_sql(name=table_name, con=conn, if_exists='replace', index=False, chunksize = 10000, method = 'multi')\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a9438d-6fd0-42e0-b553-d4692b4f0969",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Data successfully written to at: \", now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea059cf-c55b-48c3-afc6-5e5a07812650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 853924548617393,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "rfm_retail_clusters",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
