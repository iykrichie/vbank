{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9792760-b1cf-41a6-a5c5-9a4c23d31e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Healthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea29f672-37dc-434c-922b-e243ad483f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import lit\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d16ec7-d5e4-4ad7-9351-7ac0158bc85b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load in credentials\n",
    "with open('/Workspace/Credentials/db_data.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "# MIFOS_DB\n",
    "mifos_driver = \"org.mariadb.jdbc.Driver\"\n",
    "mifos_database_host = data['mifos']['host']\n",
    "mifos_database_port = \"3306\" # update if you use a non-default port\n",
    "mifos_database_name = data['mifos']['database']\n",
    "mifos_user=data['mifos']['user']\n",
    "mifos_password = data['mifos']['passwd']\n",
    "mifos_url = f\"jdbc:mysql://{mifos_database_host}:{mifos_database_port}/{mifos_database_name}\"\n",
    "\n",
    "# REDSHIFT_DWH\n",
    "redshift_driver = \"org.postgresql.Driver\"\n",
    "redshift_host = data['redshift']['host']\n",
    "redshift_db_name = data['redshift']['database']\n",
    "redshift_user = data['redshift']['user']\n",
    "redshift_password = data['redshift']['passwd']\n",
    "redshift_url = f\"jdbc:postgresql://{redshift_host}:5439/{redshift_db_name}\"\n",
    "\n",
    "\n",
    "# redshift connection for dask\n",
    "redshift_conn = f'postgresql://{redshift_user}:{redshift_password}@{redshift_host}:5439/{redshift_db_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594a2835-2cf8-4dae-8305-cf7e57e93960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Specify the target Redshift table\n",
    "redshift_table = 'balance.wallet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d37e42-bba9-48e0-9bce-6483bd9f2e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "two_days_ago =  (datetime.today() - timedelta(days = 2)).strftime('%Y-%m-%d')\n",
    "yesterday =  (datetime.today() - timedelta(days = 1)).strftime('%Y-%m-%d')\n",
    "year =  (datetime.today() - timedelta(days = 1)).strftime('%Y')\n",
    "print(year)\n",
    "print(yesterday)\n",
    "print(two_days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968d4f8b-7ecc-454d-ba2f-6548e12e8b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#yesterday = '2024-07-04' \n",
    "#two_days_ago = '2024-07-03' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12314f4-308c-47c8-a9dc-eda0e3b74a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define function to extract data from dbs\n",
    "def extract_function(query, url, user, password):\n",
    "    df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"url\", url) \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .option(\"connectTimeout\", \"1000\") \\\n",
    "                .option(\"treatEmptyValueAsNulls\",\"true\") \\\n",
    "                .option(\"maxRowsInMemory\",200) \\\n",
    "                .load()\n",
    "    return df\n",
    "\n",
    "\n",
    "## For Mifos\n",
    "sql = f'''\n",
    "SELECT msa.id AS account_id, msa.account_no, msa.client_id, msa.product_id, '{yesterday}' AS date, CAST(0.0 AS FLOAT) AS closing_balance\n",
    "FROM m_savings_account msa\n",
    "WHERE msa.product_id > 36\n",
    "AND msa.approvedon_date <= '{yesterday}'\n",
    "ORDER BY msa.id\n",
    "'''       \n",
    "#                  \n",
    "# apply function\n",
    "mifos_df = extract_function(query=sql, url=mifos_url, user=mifos_user, password=mifos_password)\n",
    "# Convert Pyspark DF to pandas DF, then to List\n",
    "mifos_df.persist()\n",
    "mifos_pdf = mifos_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbad83a-649a-4dc9-a653-fab4e72bb545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mifos_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d09a73b-8ed5-43b0-b600-95ad17fba6eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mifos_pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ca7c0d-f0a2-44a0-b198-7f4de11d5d91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load data to data warehouses\n",
    "mifos_pdf = mifos_df.toPandas()\n",
    "# Convert Pandas DataFrame to Dask DataFrame\n",
    "mifos_ddf = dd.from_pandas(mifos_pdf, npartitions=30)  # Specify the desired number of partitions\n",
    "\n",
    "\n",
    "# load data to redshift datawarehouse\n",
    "mifos_ddf.to_sql(redshift_table, redshift_conn, index=False, if_exists='append', parallel=True, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fc9815-7502-4369-8fb2-d673ff02aed6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(yesterday)\n",
    "print(two_days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9d7afe-a733-422e-8a54-b955778483e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:5439/{redshift_db_name}')\n",
    "\n",
    "sql = f'''\n",
    "UPDATE \"{redshift_table}\" AS f\n",
    "SET closing_balance = t.closing_balance\n",
    "FROM (\n",
    "\tselect stm.account_id, cast(stm.running_balance as FLOAT) as closing_balance\n",
    "\tfrom \"statement.{year}_acc_stmt\" stm\n",
    "\twhere stm.transaction_id  in (\n",
    "\t\tselect MAX(stm.transaction_id)\n",
    "\t\tfrom \"statement.{year}_acc_stmt\" stm\n",
    "\t\twhere left(stm.transaction_date,10) <= '{yesterday}'\n",
    "\t\tand stm.account_id  IN (\n",
    "\t\t\tselect account_id\n",
    "\t\t\tfrom \"{redshift_table}\"\n",
    "\t\t\twhere date = '{yesterday}'\n",
    "\t\t)\n",
    "\t\tgroup by stm.account_id\n",
    ")\n",
    ") AS t\n",
    "WHERE f.account_id = t.account_id\n",
    "and f.date = '{yesterday}'\n",
    "'''\n",
    "\n",
    "with engine.begin() as con:     \n",
    "    con.execute(sql)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21812587-5d44-4d68-9bdc-3c2decfcfa66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:5439/{redshift_db_name}')\n",
    "\n",
    "sql = f'''\n",
    "UPDATE \"{redshift_table}\" AS f\n",
    "SET closing_balance = t.closing_balance\n",
    "FROM (\n",
    "    select account_id, cast(closing_balance as FLOAT) as closing_balance\n",
    "    from \"{redshift_table}\"\n",
    "    where date = '{two_days_ago}'\n",
    "    ) AS t\n",
    "WHERE f.account_id = t.account_id\n",
    "and f.date = '{yesterday}'\n",
    "AND f.closing_balance = 0\n",
    "'''\n",
    "\n",
    "with engine.begin() as con:     \n",
    "    con.execute(sql)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0ced2c-dd0f-4e9f-b333-aff154093912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Cleaned\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "print('cleared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515e2e3d-43e6-4e65-aad3-df28f308036a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc5c346-d37f-4f56-84be-fa136a0d1707",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "n = 3\n",
    "while n >= 2:\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    two_days_ago =  (datetime.today() - timedelta(days = n)).strftime('%Y-%m-%d')\n",
    "    yesterday =  (datetime.today() - timedelta(days = n-1)).strftime('%Y-%m-%d')\n",
    "    year =  (datetime.today() - timedelta(days = n)).strftime('%Y')\n",
    "    print(year)\n",
    "    print(yesterday)\n",
    "    print(two_days_ago)\n",
    "\n",
    "    print(yesterday, \" ended\")\n",
    "    print('\\n\\n')\n",
    "    n -=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a46063a-01cb-4fc5-9cdf-a3bd10a28acf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Where n is the last day in the transactions table\n",
    "n = 8\n",
    "while n >= 2:\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    two_days_ago =  (datetime.today() - timedelta(days = n)).strftime('%Y-%m-%d')\n",
    "    yesterday =  (datetime.today() - timedelta(days = n-1)).strftime('%Y-%m-%d')\n",
    "    year =  (datetime.today() - timedelta(days = n)).strftime('%Y')\n",
    "    print(year)\n",
    "    print(yesterday)\n",
    "    print(two_days_ago)\n",
    "\n",
    "\n",
    "    # Define function to extract data from dbs\n",
    "    def extract_function(query, url, user, password):\n",
    "        df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"query\", query) \\\n",
    "                    .option(\"url\", url) \\\n",
    "                    .option(\"user\", user) \\\n",
    "                    .option(\"password\", password) \\\n",
    "                    .option(\"connectTimeout\", \"1000\") \\\n",
    "                    .option(\"treatEmptyValueAsNulls\",\"true\") \\\n",
    "                    .option(\"maxRowsInMemory\",200) \\\n",
    "                    .load()\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## For Mifos\n",
    "    sql = f'''\n",
    "    SELECT msa.id AS account_id, msa.account_no, msa.client_id, msa.product_id, '{yesterday}' AS date, CAST(0.0 AS FLOAT) AS closing_balance\n",
    "    FROM m_savings_account msa\n",
    "    WHERE msa.product_id > 36\n",
    "    AND msa.approvedon_date <= '{yesterday}'\n",
    "    ORDER BY msa.id\n",
    "    '''       \n",
    "    #                  \n",
    "    # apply function\n",
    "    mifos_df = extract_function(query=sql, url=mifos_url, user=mifos_user, password=mifos_password)\n",
    "    # Convert Pyspark DF to pandas DF, then to List\n",
    "    mifos_df.persist()\n",
    "    mifos_pdf = mifos_df.toPandas()\n",
    "\n",
    "\n",
    "    mifos_pdf.count()\n",
    "\n",
    "    # Load data to data warehouses\n",
    "    mifos_pdf = mifos_df.toPandas()\n",
    "    # Convert Pandas DataFrame to Dask DataFrame\n",
    "    mifos_ddf = dd.from_pandas(mifos_pdf, npartitions=20)  # Specify the desired number of partitions\n",
    "\n",
    "\n",
    "    # load data to redshift datawarehouse\n",
    "    mifos_ddf.to_sql(redshift_table, redshift_conn, index=False, if_exists='append', parallel=True, method='multi')\n",
    "\n",
    "\n",
    "    engine = create_engine(f'postgresql+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:5439/{redshift_db_name}')\n",
    "\n",
    "    sql = f'''\n",
    "    UPDATE \"{redshift_table}\" AS f\n",
    "    SET closing_balance = t.closing_balance\n",
    "    FROM (\n",
    "        select stm.account_id, cast(stm.running_balance as FLOAT) as closing_balance\n",
    "        from \"statement.{year}_acc_stmt\" stm\n",
    "        where stm.transaction_id  in (\n",
    "            select MAX(stm.transaction_id)\n",
    "            from \"statement.{year}_acc_stmt\" stm\n",
    "            where left(stm.transaction_date,10) <= '{yesterday}'\n",
    "            and stm.account_id  IN (\n",
    "                select account_id\n",
    "                from \"{redshift_table}\"\n",
    "                where date = '{yesterday}'\n",
    "            )\n",
    "            group by stm.account_id\n",
    "    )\n",
    "    ) AS t\n",
    "    WHERE f.account_id = t.account_id\n",
    "    and f.date = '{yesterday}'\n",
    "    '''\n",
    "\n",
    "    with engine.begin() as con:     \n",
    "        con.execute(sql)    \n",
    "        \n",
    "\n",
    "\n",
    "    engine = create_engine(f'postgresql+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:5439/{redshift_db_name}')\n",
    "\n",
    "    sql = f'''\n",
    "    UPDATE \"{redshift_table}\" AS f\n",
    "    SET closing_balance = t.closing_balance\n",
    "    FROM (\n",
    "        select account_id, cast(closing_balance as FLOAT) as closing_balance\n",
    "        from \"{redshift_table}\"\n",
    "        where date = '{two_days_ago}'\n",
    "        ) AS t\n",
    "    WHERE f.account_id = t.account_id\n",
    "    and f.date = '{yesterday}'\n",
    "    AND f.closing_balance = 0\n",
    "    '''\n",
    "\n",
    "    with engine.begin() as con:     \n",
    "        con.execute(sql)    \n",
    "    \n",
    "    print(yesterday, \" ended\")\n",
    "    print('\\n\\n')\n",
    "    n -=1\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Cleaned\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "print('cleared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b784c42-6685-46c9-a6d6-79b427f377de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wallet",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
