{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b145176-22f7-41ee-81ca-44151d7d44ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Client Demographs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d987dd-3a88-482c-b7ce-05a5a3059eea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Description\n",
    "This script sets up a connection to an Amazon Redshift database using SQLAlchemy, configures Pandas display options, and calculates various date and time values. Below is a summary of the code with explanations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a69bd7-ca66-4509-a159-bc3e4bb2a031",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Modules:\n",
    "\n",
    "- `create_engine` from `sqlalchemy` to establish a database connection.\n",
    "- `pandas` (pd) for data manipulation and analysis.\n",
    "- `numpy` (np) for numerical operations (although not used in this script).\n",
    "- `json` to handle JSON files.\n",
    "- `datetime` and `timedelta` from `datetime` to work with dates and times.\n",
    "\n",
    "### Loading Credentials:\n",
    "\n",
    "- Reads database credentials from a JSON file located at `/Workspace/Credentials/db_data.json`.\n",
    "\n",
    "### Database Connection:\n",
    "\n",
    "- Uses the extracted credentials to create a connection string for Amazon Redshift using `sqlalchemy.create_engine`.\n",
    "\n",
    "### Pandas Configuration:\n",
    "\n",
    "- Configures Pandas to display float values with two decimal places.\n",
    "\n",
    "### Date Calculations:\n",
    "\n",
    "- Calculates today's date, yesterday's date, the date 14 days ago, and the current date and time.\n",
    "- Prints these dates and times for reference.\n",
    "\n",
    "### Time Calculations:\n",
    "\n",
    "- Calculates the date and time 30 minutes ago and prints the range from 30 minutes ago to the current time.\n",
    "\n",
    "This script provides a basic setup for connecting to a database, configuring Pandas, and performing simple date and time calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98b1beb-8522-4866-b92d-d5a64bc77e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load database credentials from a JSON file\n",
    "with open('/Workspace/Credentials/db_data.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "# Extract Redshift credentials from the loaded JSON data\n",
    "host = data['redshift']['host']\n",
    "user = data['redshift']['user']\n",
    "passwd = data['redshift']['passwd']\n",
    "database = data['redshift']['database']\n",
    "\n",
    "# Create a connection engine to the Redshift database\n",
    "conn = create_engine(f\"postgresql+psycopg2://{user}:{passwd}@{host}:5439/{database}\")\n",
    "\n",
    "# Set Pandas to display floats with two decimal places\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Get today's date and yesterday's date in 'YYYY-MM-DD' format\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "yesterday =  (datetime.today() - timedelta(days = 1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Print today's and yesterday's dates\n",
    "print(today)\n",
    "print(yesterday)\n",
    "\n",
    "# Calculate and print the date 14 days ago\n",
    "last_2_wks = (datetime.today() - timedelta(days = 14)).strftime('%Y-%m-%d')\n",
    "print('------------------------------------')\n",
    "print(last_2_wks)\n",
    "\n",
    "# Print a newline for separation\n",
    "print('\\n')\n",
    "\n",
    "# Get the current date and time in 'YYYY-MM-DD HH:MM:SS' format\n",
    "now = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate and print the date and time 30 minutes ago, truncated to 'YYYY-MM-DD HH:MM'\n",
    "last_30_mins = (datetime.today() - timedelta(minutes = 30)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "trunc_last_30_mins = (datetime.today() - timedelta(minutes = 30)).strftime('%Y-%m-%d %H:%M')\n",
    "print(last_30_mins, 'to', now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7674dd03-faf1-4b62-9b8e-e3546c07bdf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generation demographic attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dd87ff-3d53-45a7-abc0-e16bcec6663e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The script below executes a SQL query to retrieve client demographic data from a Redshift database and loads the results into a Pandas DataFrame. Below is a summary of the code with explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2172a4e-464e-4c93-89ed-f32973e8ea86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "\n",
    "`%%time` is an IPython magic command to measure the execution time of the query.\n",
    "\n",
    "## SQL Query:\n",
    "\n",
    "A multi-line string containing the SQL query is defined using triple quotes (`'''`). The query retrieves client demographic data from the `dwh_all_clients` table, and joins it with the `dwh_clients_bvn_data` table on `client_id`.\n",
    "\n",
    "## SELECT Clause:\n",
    "\n",
    "- Retrieves distinct client details such as `client_id`, `client_name`, `mobile_number`, `bvn_phone_no`, `email_address`, `state`, and other demographic details from the joined tables.\n",
    "- Includes a `CASE` statement to classify clients into generational cohorts based on their birth year.\n",
    "\n",
    "## FROM Clause:\n",
    "\n",
    "- Specifies the `dwh_all_clients` table as the main table.\n",
    "- Uses a `LEFT OUTER JOIN` to include data from the `dwh_clients_bvn_data` table where available.\n",
    "\n",
    "## WHERE Clause:\n",
    "\n",
    "- Filters out clients whose status is 'closed'.\n",
    "\n",
    "## GROUP BY Clause:\n",
    "\n",
    "- Groups the results by multiple client attributes to ensure unique client records.\n",
    "\n",
    "## Pandas DataFrame:\n",
    "\n",
    "- `pd.read_sql_query` executes the SQL query and loads the result into a Pandas DataFrame `rcdem`.\n",
    "- `conn` is the database connection engine created earlier.\n",
    "\n",
    "## Execution Time Measurement:\n",
    "\n",
    "- The `%%time` magic command outputs the time taken to execute the query and load the data into the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfc01de-cf3b-4f1c-8b02-ad95bd194fb0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rcdem = pd.read_sql_query(f'''\n",
    "-- CLIENT DEMOGRAPH\n",
    "\n",
    "SELECT\n",
    "    DISTINCT dac.client_id,\n",
    "    dac.client_name,\n",
    "    dac.mobile_number,\n",
    "    dcbd.bvn_phone_no AS bvn_phone_no,\n",
    "    dac.email_address,\n",
    "    dac.state,\n",
    "    dcbd.bvn_email bvn_email,\n",
    "    dcbd.bvn,\n",
    "    dcbd.bvn_gender AS gender,\n",
    "    dcbd.bvn_dob AS date_of_birth,\n",
    "    CASE\n",
    "        WHEN dcbd.bvn_dob IS NULL THEN 'Not Found'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) < '1945' THEN 'Silent Generation'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) BETWEEN '1946' AND '1964' THEN 'Baby Boomers'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) BETWEEN '1965' AND '1979' THEN 'Generation X'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) BETWEEN '1980' AND '1994' THEN 'Millennials'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) BETWEEN '1995' AND '2012' THEN 'Generation Z'\n",
    "        WHEN RIGHT(dcbd.bvn_dob, 4) > '2012' THEN 'Generation Alpha'\n",
    "    END AS generation,\n",
    "    dcbd.bvn_state_of_origin AS state_of_origin,\n",
    "    dcbd.bvn_state_of_residence AS residence_state,\n",
    "    dac.client_tier,\n",
    "    dac.client_category,\n",
    "   /* rb.client_id AS referral_id,\n",
    "    rb.client_name AS referral_name,\n",
    "    rb.referral_code,*/\n",
    "    MIN(dac.activation_date) AS date_onboarded,\n",
    "    CURRENT_DATE as run_date\n",
    "    \n",
    "FROM\n",
    "    dwh_all_clients dac\n",
    "-- LEFT JOIN referred_by rb ON rb.client_id = dac.referred_by_id\n",
    "LEFT OUTER JOIN dwh_clients_bvn_data dcbd ON dac.client_id = dcbd.client_id\n",
    "WHERE\n",
    "    dac.client_status != 'closed' \n",
    "GROUP BY\n",
    "    dac.client_id,\n",
    "    dac.client_name,\n",
    "    dac.mobile_number,\n",
    "    dac.email_address,\n",
    "    dcbd.bvn_email,\n",
    "    dac.state,\n",
    "    dcbd.bvn,\n",
    "    dcbd.bvn_phone_no,\n",
    "    dcbd.bvn_gender,\n",
    "    dcbd.bvn_dob,\n",
    "    dcbd.bvn_state_of_origin,\n",
    "    dcbd.bvn_state_of_residence,\n",
    "    dac.client_tier,\n",
    "    dac.client_category\n",
    "/*    rb.client_id,\n",
    "    rb.client_name,\n",
    "    rb.referral_code*/\n",
    "--  LIMIT 1000;\n",
    "\n",
    "\n",
    "\n",
    "''' , conn)\n",
    "\n",
    "\n",
    "rcdem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae4ef70-e282-4fb5-8f0d-d6249056d0a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This script below writes data from a Pandas DataFrame into an Amazon Redshift database in a single operation using the `to_sql` method with batching and multi-row insert support. The execution time of the operation is measured using the IPython `%%time` magic command.\n",
    "\n",
    "### Code Summary\n",
    "\n",
    "```python\n",
    "%%time\n",
    "rcdem.to_sql(\"dwh_clients_demograph\", conn, index=False, if_exists='replace', chunksize=30000, method='multi')\n",
    "\n",
    "print(\"run completed successfully on \" + now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4315cec-819d-47b9-965b-c154a376e12e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load demographic attributes to Redshift table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea85704-d493-448a-9b92-936673cf8ed8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "rcdem.to_sql(\"dwh_clients_demograph\", conn, index=False, if_exists='replace', chunksize=30000, method='multi')\n",
    "\n",
    "print(\"run completed successfully on \" + now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756a7612-51ed-4ac8-a74d-5feb08755ed0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This script writes data from a Pandas DataFrame into an Amazon Redshift database in iterative batches. This approach helps manage memory usage and ensures efficient data transfer for large datasets.\n",
    "\n",
    "Code Summary\n",
    "\n",
    "```python\n",
    "# Define the batch size for iterative writing\n",
    "batch_size = 10000\n",
    "\n",
    "# Loop through the DataFrame in increments of batch_size\n",
    "for i in range(0, len(rcdem), batch_size):\n",
    "    # Extract a batch of data from the DataFrame\n",
    "    rcdem_batch = rcdem[i:i+batch_size]\n",
    "    \n",
    "    # Write the batch to the Redshift table 'dwh_clients_demograph'\n",
    "    rcdem_batch.to_sql(\"dwh_clients_demograph\", conn, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0b4323-f255-4f5c-86be-e1f8911dde48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Write the data in iterative batches into Redshift\n",
    "batch_size = 10000\n",
    "for i in range(0, len(rcdem), batch_size):\n",
    "    rcdem_batch = rcdem[i:i+batch_size]\n",
    "    rcdem_batch.to_sql(\"dwh_clients_demograph\", conn, index=False, if_exists='replace')\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481072b5-cd4d-45a0-81eb-ee24cea36e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa9f34c-2094-4ed7-a21e-a734dad697e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"run completed successfully on \" + now)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clients_demograph",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
