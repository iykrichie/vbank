{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599ac04f-5bf2-4dd5-8295-b1ac24966ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Classifying Sticky, Active, Inactive, and Dormant Accounts\n",
    "\n",
    "Classify Sticky, Active, Inactive and Dormant accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28de63d-8202-43bc-883c-18baa9c1e226",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Code Explanation\n",
    "1. Import necessary libraries:\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f965e2b-ead9-4b79-8ad4-f3805531c8e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Load credentials from JSON (assuming it's a valid JSON file)\n",
    "with open('/Workspace/Credentials/db_data.json', 'r') as fp:\n",
    "    credentials = json.load(fp)\n",
    "\n",
    "# Extract connection details from dictionary (assuming keys exist)\n",
    "host = credentials['redshift']['host']\n",
    "user = credentials['redshift']['user']\n",
    "passwd = credentials['redshift']['passwd']\n",
    "database = credentials['redshift']['database']\n",
    "\n",
    "# Connect to Redshift using a connection string (cleaner approach)\n",
    "conn_string = f\"postgresql+psycopg2://{user}:{passwd}@{host}:5439/{database}\"\n",
    "conn = create_engine(conn_string)\n",
    "\n",
    "# Set display format for floats (consider using a global configuration file)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Get today and yesterday's dates efficiently using pandas\n",
    "today = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "yesterday = (pd.Timestamp.today() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Calculate date for last 2 weeks using pandas (cleaner approach)\n",
    "last_2_weeks = (pd.Timestamp.today() - pd.Timedelta(days=14)).strftime('%Y-%m-%d')\n",
    "\n",
    "print('------------------------------------')\n",
    "print(last_2_weeks)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Get timestamps efficiently using pandas\n",
    "now = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "last_30_mins = (pd.Timestamp.today() - pd.Timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "trunc_last_30_mins = (pd.Timestamp.today() - pd.Timedelta(days=1)).strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "print(f\"{last_30_mins} to {now}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12028289-e4a5-4530-abf7-988d6978ed1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classifying Sticky, Active, Inactive and Dormant Accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6d7395-96ff-48a9-b92f-da93e179ff7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This code snippet below is defining the conditions for two calculated fields: `sticky` and `account_status`. \n",
    "\n",
    "1. `sticky`: It categorizes clients based on recent transaction activity. If a client has had a transaction within the last 30 days, they are labeled as 'sticky'; otherwise, they are 'non sticky'.\n",
    "\n",
    "2. `account_status`: It classifies clients based on their transaction history. If a client has had a transaction within the last 60 days, they are 'active'. If the last transaction was between 61 to 179 days ago, they are 'inactive'. Otherwise, they are labeled as 'dormant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16424d54-ff7d-4bb3-8faa-cc51e91e2f81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = '''\n",
    "SELECT\n",
    "    dac.client_id,\n",
    "    CASE\n",
    "        WHEN COUNT(dat.transaction_id) > 0 AND DATEDIFF(DAY, MAX(dat.transaction_date), CURRENT_DATE) <= 30 THEN 'sticky'\n",
    "        ELSE 'non sticky'\n",
    "    END AS sticky,\n",
    "    CASE\n",
    "        WHEN DATEDIFF(DAY, MAX(dat.transaction_date), CURRENT_DATE) <= 90 THEN 'active'\n",
    "        WHEN DATEDIFF(DAY, MAX(dat.transaction_date), CURRENT_DATE) BETWEEN 91 AND 180 THEN 'inactive'\n",
    "        ELSE 'dormant'\n",
    "    END AS account_status,\n",
    "    DATEDIFF(DAY, MAX(dat.transaction_date), CURRENT_DATE) AS days_since_last_transaction,\n",
    "    DATEDIFF(MONTH, MIN(dac.activation_date), CURRENT_DATE) AS age_banked_in_months,\n",
    "    MIN(dat.transaction_date) AS first_transaction_date,\n",
    "    MAX(dat.transaction_date) AS last_transaction_date,\n",
    "    COUNT(dat.transaction_id) AS transaction_volume,\n",
    "    SUM(dat.amount) AS total_transaction_value,\n",
    "    AVG(dat.running_balance) AS avg_bal,\n",
    "    CURRENT_DATE AS run_date\n",
    "FROM\n",
    "    dwh_all_clients dac\n",
    "LEFT JOIN dwh_all_accounts daa ON dac.client_id = daa.client_id\n",
    "LEFT JOIN dwh_all_transactions dat ON dac.client_id = dat.client_id\n",
    "WHERE\n",
    "    dat.transaction_type_enum IN ('1', '2')\n",
    "    AND dac.client_status != 'closed'\n",
    "    AND dat.transaction_date >= DATEADD(MONTH, -12, CURRENT_DATE) \n",
    "GROUP BY\n",
    "    dac.client_id\n",
    "\n",
    "'''\n",
    "\n",
    "# Execute the query using the engine and read the result into a DataFrame\n",
    "query_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the first few rows of the result\n",
    "query_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e55442d-d8b4-44d9-97e7-8195f689394c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This code snippet demonstrates how to use PySpark to process data and store it in a table in Databricks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a750144-87d1-4ddb-ad32-e3ed23e7c108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "df = query_data\n",
    "display(df)\n",
    "\n",
    "# Create a SparkSession if not already created\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Write Spark DataFrame to table in Databricks\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"vfd_databricks.default.active_dormancy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28beab5c-9965-44a2-af24-217cbb9ccd36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "query_data.to_sql(\"dwh_active_dormancy\", conn, index = False, if_exists = 'append', chunksize = 5000, method = 'multi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139dd233-526f-4d15-93f4-6dda9504aeb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"run completed successfully on {now} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51750f6d-7513-49d8-9b02-638ac712843d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 853924548617395,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "active_dormancy",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
